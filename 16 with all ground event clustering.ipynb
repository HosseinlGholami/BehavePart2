{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in the name of GOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cbook import flatten\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as matplotlib\n",
    "\n",
    "from fastdtw import fastdtw\n",
    "\n",
    "def color_map_color(value, cmap_name='coolwarm', vmin=0, vmax=10):\n",
    "    # norm = plt.Normalize(vmin, vmax)\n",
    "    norm = matplotlib.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    cmap = cm.get_cmap(cmap_name)  # PiYG\n",
    "    rgb = cmap(norm(abs(value)))[:3]  # will return rgba, we take only first 3 so we get rgb\n",
    "    color = matplotlib.colors.rgb2hex(rgb)\n",
    "    return color\n",
    "\n",
    "#extract the labels for clutering precision its for after clustering\n",
    "def time_convertor(x):\n",
    "    s,h,m=0,0,0\n",
    "    s=round(x%60,2)\n",
    "    m=int(x/60)\n",
    "    h=int(m/60)\n",
    "    m=m%60\n",
    "    time=str(h)+':'+str(m)+':'+str(s)\n",
    "    return pd.to_datetime('1970-01-01 '+time)\n",
    "\n",
    "\n",
    "# Display figures inline in Jupyter notebook\n",
    "import seaborn as sns\n",
    "# Use seaborn style defaults and set the default figure size\n",
    "sns.set(rc={'figure.figsize':(15, 5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare Dataset gyroscope\n",
    "dfg=pd.read_csv('16\\giroscopio_terra.csv')\n",
    "dfg['ts']=pd.to_datetime(dfg.apply(lambda x:(round((x.uptimeNanos-dfg.uptimeNanos[0])/1000000)*1000000),axis=1))\n",
    "dm=dfg['ts']\n",
    "dfg=dfg.drop('uptimeNanos',1).drop('timestamp',1)\n",
    "dfg=dfg.set_index('ts').rename(columns={'x': 'g-x','y': 'g-y','z': 'g-z'})\n",
    "#prepare Dataset accelarator\n",
    "dfa=pd.read_csv('16\\\\acelerometro_terra.csv')\n",
    "dfa['ts']=dm\n",
    "dfa=dfa.drop('uptimeNanos',1).drop('timestamp',1)\n",
    "dfa=dfa.set_index('ts').rename(columns={'x': 'a-x','y': 'a-y','z': 'a-z'})\n",
    "df1 = pd.concat([dfa, dfg], axis=1, join='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare Dataset gyroscope\n",
    "dfg=pd.read_csv('17\\giroscopio_terra.csv')\n",
    "dfg['ts']=pd.to_datetime(dfg.apply(lambda x:(round((x.uptimeNanos-dfg.uptimeNanos[0])/1000000)*1000000),axis=1))\n",
    "dm=dfg['ts']\n",
    "dfg=dfg.drop('uptimeNanos',1).drop('timestamp',1)\n",
    "dfg=dfg.set_index('ts').rename(columns={'x': 'g-x','y': 'g-y','z': 'g-z'})\n",
    "#prepare Dataset accelarator\n",
    "dfa=pd.read_csv('17\\\\acelerometro_terra.csv')\n",
    "dfa['ts']=dm\n",
    "dfa=dfa.drop('uptimeNanos',1).drop('timestamp',1)\n",
    "dfa=dfa.set_index('ts').rename(columns={'x': 'a-x','y': 'a-y','z': 'a-z'})\n",
    "df2 = pd.concat([dfa, dfg], axis=1, join='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare Dataset gyroscope\n",
    "dfg=pd.read_csv('20\\\\giroscopio_terra.csv')\n",
    "dfg['ts']=pd.to_datetime(dfg.apply(lambda x:(round((x.uptimeNanos-dfg.uptimeNanos[0])/1000000)*1000000),axis=1))\n",
    "dm=dfg['ts']\n",
    "dfg=dfg.drop('uptimeNanos',1).drop('timestamp',1)\n",
    "dfg=dfg.set_index('ts').rename(columns={'x': 'g-x','y': 'g-y','z': 'g-z'})\n",
    "#prepare Dataset accelarator\n",
    "dfa=pd.read_csv('20\\\\acelerometro_terra.csv')\n",
    "dfa['ts']=dm\n",
    "dfa=dfa.drop('uptimeNanos',1).drop('timestamp',1)\n",
    "dfa=dfa.set_index('ts').rename(columns={'x': 'a-x','y': 'a-y','z': 'a-z'})\n",
    "df3 = pd.concat([dfa, dfg], axis=1, join='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare Dataset gyroscope\n",
    "dfg=pd.read_csv('21\\giroscopio_terra.csv')\n",
    "dfg['ts']=pd.to_datetime(dfg.apply(lambda x:(round((x.uptimeNanos-dfg.uptimeNanos[0])/1000000)*1000000),axis=1))\n",
    "dm=dfg['ts']\n",
    "dfg=dfg.drop('uptimeNanos',1).drop('timestamp',1)\n",
    "dfg=dfg.set_index('ts').rename(columns={'x': 'g-x','y': 'g-y','z': 'g-z'})\n",
    "#prepare Dataset accelarator\n",
    "dfa=pd.read_csv('21\\\\acelerometro_terra.csv')\n",
    "dfa['ts']=dm\n",
    "dfa=dfa.drop('uptimeNanos',1).drop('timestamp',1)\n",
    "dfa=dfa.set_index('ts').rename(columns={'x': 'a-x','y': 'a-y','z': 'a-z'})\n",
    "df4 = pd.concat([dfa, dfg], axis=1, join='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare Labeled dataset on 16\n",
    "dfl=pd.read_csv('16\\\\groundTruth.csv')\n",
    "dfl['length']=dfl.en-dfl.st\n",
    "lmean=dfl.length.mean()\n",
    "lstd=dfl.length.std()\n",
    "\n",
    "dfl['st_time']=dfl.apply(lambda x:time_convertor(x.st) ,axis=1 )\n",
    "dfl['en_time']=dfl.apply(lambda x:time_convertor(x.en) ,axis=1 )\n",
    "dfl=dfl.drop('st',1).drop('en',1).drop('length',1)\n",
    "evented_label_1=list()\n",
    "for i in range(len(dfl)):\n",
    "    evented_label_1.append((dfl.iloc[i][0],dfl.iloc[i][1],dfl.iloc[i][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare Labeled dataset on 17\n",
    "dfl=pd.read_csv('17\\\\groundTruth.csv')\n",
    "dfl['length']=dfl.en-dfl.st\n",
    "lmean=dfl.length.mean()\n",
    "lstd=dfl.length.std()\n",
    "\n",
    "dfl['st_time']=dfl.apply(lambda x:time_convertor(x.st) ,axis=1 )\n",
    "dfl['en_time']=dfl.apply(lambda x:time_convertor(x.en) ,axis=1 )\n",
    "dfl=dfl.drop('st',1).drop('en',1).drop('length',1)\n",
    "evented_label_2=list()\n",
    "for i in range(len(dfl)):\n",
    "    evented_label_2.append((dfl.iloc[i][0],dfl.iloc[i][1],dfl.iloc[i][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare Labeled dataset on 20\n",
    "dfl=pd.read_csv('20\\\\groundTruth.csv')\n",
    "dfl['length']=dfl.en-dfl.st\n",
    "lmean=dfl.length.mean()\n",
    "lstd=dfl.length.std()\n",
    "\n",
    "dfl['st_time']=dfl.apply(lambda x:time_convertor(x.st) ,axis=1 )\n",
    "dfl['en_time']=dfl.apply(lambda x:time_convertor(x.en) ,axis=1 )\n",
    "dfl=dfl.drop('st',1).drop('en',1).drop('length',1)\n",
    "evented_label_3=list()\n",
    "for i in range(len(dfl)):\n",
    "    evented_label_3.append((dfl.iloc[i][0],dfl.iloc[i][1],dfl.iloc[i][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare Labeled dataset on 21\n",
    "dfl=pd.read_csv('17\\\\groundTruth.csv')\n",
    "dfl['length']=dfl.en-dfl.st\n",
    "lmean=dfl.length.mean()\n",
    "lstd=dfl.length.std()\n",
    "\n",
    "dfl['st_time']=dfl.apply(lambda x:time_convertor(x.st) ,axis=1 )\n",
    "dfl['en_time']=dfl.apply(lambda x:time_convertor(x.en) ,axis=1 )\n",
    "dfl=dfl.drop('st',1).drop('en',1).drop('length',1)\n",
    "evented_label_4=list()\n",
    "for i in range(len(dfl)):\n",
    "    evented_label_4.append((dfl.iloc[i][0],dfl.iloc[i][1],dfl.iloc[i][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"temp_event_on16.txt\", \"rb\") as fp:\n",
    "    te = pickle.load(fp)\n",
    "with open(\"distance_on16.txt\", \"rb\") as fp:\n",
    "    dc = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disstance calculatoin  92.92  %   is complete on  1\n"
     ]
    }
   ],
   "source": [
    "#Dont RUN this part\n",
    "#clustering precision\n",
    "def eveluate_ground_distance_from_temp_eventt(df,evented_label,temp_event,Name):\n",
    "    n=0\n",
    "    k=len(temp_event)*len(evented_label)\n",
    "    labels_distance=list()\n",
    "\n",
    "    for tag,st,en in evented_label:\n",
    "        dummy=list()\n",
    "        for t_e in temp_event:\n",
    "            clear_output(wait=True)\n",
    "            n=n+1\n",
    "            print('disstance calculatoin ',round((n/k)*100,2),' %   is complete on ',Name )\n",
    "            dist,_=fastdtw(df[st:en],t_e)\n",
    "            dummy.append(dist)\n",
    "        labels_distance.append((tag,dummy))\n",
    "    return labels_distance\n",
    "\n",
    "t1=time.time()\n",
    "ld1=eveluate_ground_distance_from_temp_eventt(df1,evented_label_1,te,'1')\n",
    "with open(\"ld1_on16.txt\", \"wb\") as fp:\n",
    "    pickle.dump(ld1, fp)\n",
    "\n",
    "ld2=eveluate_ground_distance_from_temp_eventt(df2,evented_label_2,te,'2')\n",
    "with open(\"ld2_on16.txt\", \"wb\") as fp:\n",
    "    pickle.dump(ld1, fp)\n",
    "\n",
    "ld3=eveluate_ground_distance_from_temp_eventt(df3,evented_label_3,te,'3')\n",
    "with open(\"ld3_on16.txt\", \"wb\") as fp:\n",
    "    pickle.dump(ld3, fp)\n",
    "    \n",
    "ld4=eveluate_ground_distance_from_temp_eventt(df4,evented_label_4,te,'4')\n",
    "with open(\"ld4_on16.txt\", \"wb\") as fp:\n",
    "    pickle.dump(ld4, fp)\n",
    "t2=time.time()\n",
    "print(round((t2-t1)/60) ,'min has time for calculaton distant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "ld=ld1.copy()\n",
    "ld.append(ld2)\n",
    "ld.append(ld3)\n",
    "ld.append(ld4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "#plot and optimize the number of the cluster and the linkage method\n",
    "def creat_Symmetric_matrix(file):  \n",
    "    MTX=deepcopy(file)\n",
    "    for i in range(len(MTX)):\n",
    "        while len(MTX[i])!=len(MTX):\n",
    "            MTX[i].append(0)\n",
    "    #make it Symmetry from diameter\n",
    "    arr=np.array(MTX)\n",
    "    dis=np.array(MTX)\n",
    "    for i in range(len(arr)):\n",
    "        for j in range(len(arr)):\n",
    "            if (i>j):\n",
    "                dis[j][i]=arr[i][j]\n",
    "    return dis\n",
    "\n",
    "def Mean(dprecision):\n",
    "    return np.mean(list(dict(dprecision).values()))\n",
    "\n",
    "def Oploter(number_of_cluster,linkage_method):\n",
    "    #calculate the precision of clustering by labeled data \n",
    "    exploratory_analysis=list()\n",
    "    for el in ld:\n",
    "        midle_list=list()\n",
    "        label_tag =el[0]\n",
    "        label_distance=el[1].copy()\n",
    "        label_distance.append(0)\n",
    "        distance_matrix=dc.copy()\n",
    "        distance_matrix.append(label_distance)\n",
    "        DMX=creat_Symmetric_matrix(distance_matrix)\n",
    "        Labels = AgglomerativeClustering(n_clusters=number_of_cluster, affinity='precomputed', linkage = linkage_method).fit(DMX).labels_\n",
    "\n",
    "        #extract lable  and labele it from the item we put at the end\n",
    "        for i , x in enumerate(Labels):\n",
    "            if i!=(len(Labels)-1):\n",
    "                if x==Labels[-1]:\n",
    "                    midle_list.append(i)\n",
    "        exploratory_analysis.append([label_tag,midle_list])\n",
    "    actg=dict()\n",
    "    for ev,x in exploratory_analysis:\n",
    "        if ev in actg:\n",
    "            actg[ev].append(x)\n",
    "        else:\n",
    "            actg.update({ev:[x]})\n",
    "    #calculate the Probibility of being event of temp_event\n",
    "    Prob_threshold=0.1\n",
    "    precision=list()\n",
    "    for event_name in actg:\n",
    "        flat_list=list(dict.fromkeys(flatten(actg[event_name])))\n",
    "        dprecision=list()\n",
    "        for S in flat_list:\n",
    "            x=0\n",
    "            Number=len(actg[event_name])\n",
    "            for i in range(Number):\n",
    "                if S in actg[event_name][i]:\n",
    "                    x=x+1\n",
    "            Probability=x/Number\n",
    "            if Probability > Prob_threshold:\n",
    "                dprecision.append((S,Probability))\n",
    "        precision.append((event_name,len(dprecision),Mean(dprecision),dprecision))\n",
    "    return precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linkage method average\n",
    "linkage_mth='average'\n",
    "Ya=list()\n",
    "t1=time.time()\n",
    "for num_cl in range(1,500,50):\n",
    "    X=Oploter(num_cl,linkage_mth)\n",
    "    Ya.append(X)\n",
    "t2=time.time()\n",
    "print(round((t2-t1)/60) ,'min has time for calculaton distant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

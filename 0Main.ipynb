{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in the name of god"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cbook import flatten\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as matplotlib\n",
    "\n",
    "from fastdtw import fastdtw\n",
    "\n",
    "def color_map_color(value, cmap_name='coolwarm', vmin=0, vmax=10):\n",
    "    # norm = plt.Normalize(vmin, vmax)\n",
    "    norm = matplotlib.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    cmap = cm.get_cmap(cmap_name)  # PiYG\n",
    "    rgb = cmap(norm(abs(value)))[:3]  # will return rgba, we take only first 3 so we get rgb\n",
    "    color = matplotlib.colors.rgb2hex(rgb)\n",
    "    return color\n",
    "cl=['r','g','b','c','m','y','k']\n",
    "color=dict()\n",
    "for i,el in enumerate(cl):\n",
    "    color.update({i:el})\n",
    "#extract the labels for clutering precision its for after clustering\n",
    "def time_convertor(x):\n",
    "    s,h,m=0,0,0\n",
    "    s=round(x%60,2)\n",
    "    m=int(x/60)\n",
    "    h=int(m/60)\n",
    "    m=m%60\n",
    "    time=str(h)+':'+str(m)+':'+str(s)\n",
    "    return pd.to_datetime('1970-01-01 '+time)\n",
    "\n",
    "\n",
    "# Display figures inline in Jupyter notebook\n",
    "import seaborn as sns\n",
    "# Use seaborn style defaults and set the default figure size\n",
    "sns.set(rc={'figure.figsize':(15, 5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_read\n",
    "def read_data(number):\n",
    "    #prepare Dataset gyroscope\n",
    "    dfg=pd.read_csv(f'{number}\\\\giroscopio_terra.csv')\n",
    "    dfg['ts']=pd.to_datetime(dfg.apply(lambda x:(round((x.uptimeNanos-dfg.uptimeNanos[0])/1000000)*1000000),axis=1))\n",
    "    dm=dfg['ts']\n",
    "    dfg=dfg.drop('uptimeNanos',1).drop('timestamp',1)\n",
    "    dfg=dfg.set_index('ts').rename(columns={'x': 'g-x','y': 'g-y','z': 'g-z'})\n",
    "    #prepare Dataset accelarator\n",
    "    dfa=pd.read_csv(f'{number}\\\\acelerometro_terra.csv')\n",
    "    dfa['ts']=dm\n",
    "    dfa=dfa.drop('uptimeNanos',1).drop('timestamp',1)\n",
    "    dfa=dfa.set_index('ts').rename(columns={'x': 'a-x','y': 'a-y','z': 'a-z'})\n",
    "    dfg['g-x']=dfg.apply(lambda x:(x[0]-min(dfg['g-x']))/(max(dfg['g-x'])-min(dfg['g-x'])),axis=1)\n",
    "    dfg['g-y']=dfg.apply(lambda x:(x[1]-min(dfg['g-y']))/(max(dfg['g-y'])-min(dfg['g-y'])),axis=1)\n",
    "    dfg['g-z']=dfg.apply(lambda x:(x[2]-min(dfg['g-z']))/(max(dfg['g-z'])-min(dfg['g-z'])),axis=1)\n",
    "    dfa['a-x']=dfa.apply(lambda x:(x[0]-min(dfa['a-x']))/(max(dfa['a-x'])-min(dfa['a-x'])),axis=1)\n",
    "    dfa['a-y']=dfa.apply(lambda x:(x[1]-min(dfa['a-y']))/(max(dfa['a-y'])-min(dfa['a-y'])),axis=1)\n",
    "    dfa['a-z']=dfa.apply(lambda x:(x[2]-min(dfa['a-z']))/(max(dfa['a-z'])-min(dfa['a-z'])),axis=1)\n",
    "    return pd.concat([dfa, dfg], axis=1, join='outer')\n",
    "dfRaw=list()\n",
    "for num in [16,17,20,21]:\n",
    "    dfRaw.append((num,read_data(num)))\n",
    "with open(\"normalized_data.txt\", \"wb\") as fp:\n",
    "    pickle.dump(dfRaw, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"normalized_data.txt\", \"rb\") as fp:\n",
    "        dfRaw = pickle.load(fp)\n",
    "        \n",
    "def get_label(number,df):\n",
    "    label_event_lenght=dict()\n",
    "    df=pd.read_csv(f'{number}\\\\groundTruth.csv')\n",
    "    df['length']=df.en-df.st\n",
    "    for event , dft in df.groupby('evento'):\n",
    "        if (event in label_event_lenght):\n",
    "            label_event_lenght.update({event : label_event_lenght[event]+list(dft.length) })\n",
    "        else:\n",
    "            label_event_lenght.update({event : list(dft.length)})\n",
    "\n",
    "    df['st_time']=df.apply(lambda x:time_convertor(x.st) ,axis=1 )\n",
    "    df['en_time']=df.apply(lambda x:time_convertor(x.en) ,axis=1 )\n",
    "    df=df.drop('st',1).drop('en',1).drop('length',1)\n",
    "    evented_label=list()\n",
    "    for i in range(len(df)):\n",
    "        evented_label.append((df.iloc[i][0],df.iloc[i][1],df.iloc[i][2]))\n",
    "    eve=pd.DataFrame(evented_label)\n",
    "    eve[3]=[f'evented_label_{number}' for x in evented_label]\n",
    "    return eve\n",
    "Ev=[get_label(dfRaw[x][0],dfRaw[x][1]) for x in range(len(dfRaw))]\n",
    "evented_label_All=pd.concat(Ev).reset_index().drop('index',1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate=(dfRaw[0][1].index[11]-dfRaw[0][1].index[10]).total_seconds()\n",
    "\n",
    "print(f\"rate is :{rate}\")\n",
    "pd.DataFrame([(x[0],round(((x[2]-x[1]).total_seconds())/rate)) for x in evented_label_All.iloc]).groupby(0).agg({1:\"max\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supervised\n",
    "window={226:['curva_direita_agressiva','curva_esquerda_agressiva','aceleracao_agressiva'],\n",
    "        126:['troca_faixa_direita_agressiva','troca_faixa_esquerda_agressiva'],\n",
    "        186:['freada_agressiva']\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dont RUN this part\n",
    "#we should optimize M\n",
    "#lengh of random event\n",
    "import random\n",
    "random.seed(123)\n",
    "def TempEvent_genarator(df,L,i):\n",
    "    temp_event=list()\n",
    "    pointer=0\n",
    "    stop=False\n",
    "    #go on timeserise for event extraction\n",
    "    while stop!=True:\n",
    "        M=round(random.gauss(0.1,1)*L)\n",
    "        clear_output(wait=True)\n",
    "        pointer=pointer + M\n",
    "        l_min=pointer-int(L/2)\n",
    "        l_max=pointer+int(L/2)\n",
    "        if l_max<0:\n",
    "            l_max=L/2\n",
    "        if (l_min<0):\n",
    "            l_min=0\n",
    "        if (l_max>len(df)):\n",
    "            l_max=len(df)\n",
    "            stop=True\n",
    "        print('extraction ',l_max/len(df),' % of ',i,' is complete')\n",
    "        temp_event.append(df[int(l_min):int(l_max)])\n",
    "    return temp_event\n",
    "\n",
    "temp_event=list()\n",
    "for l,DF in enumerate([x[1] for x in dfRaw]):\n",
    "    te=list()\n",
    "    for i,lw in enumerate(window):\n",
    "        te.append((window[lw],TempEvent_genarator(DF,lw,i+1)))\n",
    "    temp_event.append(te)\n",
    "\n",
    "with open(\"temp_event_new.txt\", \"wb\") as fp:\n",
    "    pickle.dump(temp_event, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"temp_event_new.txt\", \"rb\") as fp:\n",
    "    temp_event = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_overlap(A_start, A_end, B_start, B_end):\n",
    "    latest_start = max(A_start, B_start)\n",
    "    earliest_end = min(A_end, B_end)\n",
    "    return latest_start <= earliest_end\n",
    "\n",
    "def export_labels(tempevent,evented_label):\n",
    "    EvL=pd.DataFrame(evented_label)\n",
    "    Labels={\n",
    "          226:[],\n",
    "          126:[],\n",
    "          186:[],\n",
    "           }\n",
    "    for i,wind in enumerate(Labels):\n",
    "        label=['NAG' for x in tempevent[i][1]]\n",
    "        for name in tempevent[i][0]:\n",
    "            el=[(x[1],x[2]) for x in EvL.iloc if x[0]==name]\n",
    "            for j,te in enumerate(tempevent[i][1]):\n",
    "                    sta =te.index[0]\n",
    "                    ena =te.index[-1]\n",
    "                    for stb,enb in el:\n",
    "                        if has_overlap(sta,ena,stb,enb):\n",
    "                            label[j]=name\n",
    "        Labels[wind]=label\n",
    "    return Labels\n",
    "\n",
    "\n",
    "Data={\n",
    "      226:[],\n",
    "      126:[],\n",
    "      186:[],\n",
    "       }\n",
    "for i,temp in enumerate(temp_event):\n",
    "    TempEv={\n",
    "              226:[ x for x in temp_event[i][0][1] ],\n",
    "              126:[ x for x in temp_event[i][1][1] ],\n",
    "              186:[ x for x in temp_event[i][2][1] ],\n",
    "               }\n",
    "    Label=export_labels(temp_event[i],Ev[i])\n",
    "    #merge together\n",
    "    for wind in Data:\n",
    "        for j in range(len(TempEv[wind])):\n",
    "            Data[wind].append((Label[wind][j],TempEv[wind][j]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data[226][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dont run this part\n",
    "#calculate dist of Data from labdeled event\n",
    "t1=time.time()\n",
    "\n",
    "k,n=0,0\n",
    "num_event=dict(evented_label_All.groupby(0).size())\n",
    "for lenght in window:\n",
    "    for name in window[lenght]:\n",
    "        k=k+len(Data[lenght])*num_event[name]\n",
    "\n",
    "Data_DS={\n",
    "      226:[],\n",
    "      126:[],\n",
    "      186:[],\n",
    "       }\n",
    "\n",
    "for wind in Data:\n",
    "    for label,event in Data[wind]:\n",
    "         #---------for each Data-distance from events----------\n",
    "        Y=list()\n",
    "        for from_event,dfe in evented_label_All.groupby(0):\n",
    "            if from_event in window[wind]:\n",
    "                X=list()\n",
    "                #____for each event_distance from each label_____\n",
    "                for el in dfe.iloc:\n",
    "                    clear_output(wait=True)\n",
    "                    n=n+1\n",
    "                    print('disstance calculatoin ',round((n/k)*100,4),' %  is complete')\n",
    "                    if   el[3]=='evented_label_16':\n",
    "                        X.append(fastdtw(event,dfRaw[0][1][el[1]:el[2]])[0])\n",
    "                    elif el[3]=='evented_label_17':\n",
    "                        X.append(fastdtw(event,dfRaw[1][1][el[1]:el[2]])[0])\n",
    "                    elif el[3]=='evented_label_20':\n",
    "                        X.append(fastdtw(event,dfRaw[2][1][el[1]:el[2]])[0])\n",
    "                    elif el[3]=='evented_label_21':\n",
    "                        X.append(fastdtw(event,dfRaw[3][1][el[1]:el[2]])[0])\n",
    "                #_________________________________________________\n",
    "                Y.append((from_event,X))\n",
    "        Data_DS[wind].append((label,Y))\n",
    "        #------------------------------------------------------\n",
    "\n",
    "t2=time.time()\n",
    "with open(\"Disstance_new.txt\", \"wb\") as fp:\n",
    "    pickle.dump(Data_DS, fp)\n",
    "\n",
    "print(round((t2-t1)/60) ,'min has time for calculaton distant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Disstance_new.txt\", \"rb\") as fp:\n",
    "    Data_DS = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_DS[226][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare dataset for labeling\n",
    "TrainData=dict()\n",
    "for win in Data_DS:\n",
    "    x=Data_DS[win]\n",
    "    dumy_list=list()\n",
    "    for events in x:\n",
    "        dumy_dict=dict()\n",
    "        dumy_dict.update({'label':events[0]})\n",
    "        for name , event in events[1]:\n",
    "            for tag,number in enumerate(event):\n",
    "                dumy_dict.update({name+str(tag):number})\n",
    "        dumy_list.append(dumy_dict)\n",
    "    TrainData.update({win:pd.DataFrame(dumy_list)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TrainData[226].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize Train Data\n",
    "#Balance it \n",
    "from copy import deepcopy\n",
    "import six\n",
    "import sys\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "from id3 import Id3Estimator, export_text\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import accuracy_score, classification_report,precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "wid=226\n",
    "Max_depth=10\n",
    "\n",
    "data=TrainData[wid].drop('label',1)\n",
    "label=TrainData[wid].label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.33, random_state=42)\n",
    "\n",
    "estimator = Id3Estimator(max_depth=Max_depth, min_samples_split=1, prune=True,\n",
    "                        gain_ratio=True, min_entropy_decrease=0, is_repeating=True)\n",
    "\n",
    "estimator.fit(X_train,y_train , check_input=True)\n",
    "\n",
    "\n",
    "precision_recall_fscore_support(y_test, estimator.predict(X_test),average='macro')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disstance calculatoin  300.0  %  is complete\n",
      "356 min has time for calculaton Model\n"
     ]
    }
   ],
   "source": [
    "#dont run this part\n",
    "from copy import deepcopy\n",
    "import six\n",
    "import sys\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "from id3 import Id3Estimator, export_text\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "t1=time.time()\n",
    "n,k=0,12*50\n",
    "\n",
    "Result={\n",
    "      226:[],\n",
    "      126:[],\n",
    "      186:[],\n",
    "       }\n",
    "\n",
    "for wid in Result:\n",
    "    data=TrainData[wid].drop('label',1)\n",
    "    label=TrainData[wid].label\n",
    "    #\n",
    "    for Max_depth in range(1,13):\n",
    "        estimator = Id3Estimator(max_depth=Max_depth, min_samples_split=1, prune=True,\n",
    "                        gain_ratio=True, min_entropy_decrease=0, is_repeating=True)\n",
    "        #run it in itaraion for the best trees\n",
    "        NUMBER_OF_TEST=50\n",
    "        temp=list()\n",
    "        for NT in range(NUMBER_OF_TEST):\n",
    "            clear_output(wait=True)\n",
    "            n=n+1\n",
    "            print('disstance calculatoin ',round((n/k)*100,4),' %  is complete')\n",
    "            estimator.fit(data,label , check_input=True)\n",
    "            ACC=accuracy_score(y_true=label,y_pred=estimator.predict(data))\n",
    "            temp.append((ACC,deepcopy(estimator)))\n",
    "        temp_index=[(x[0],i) for i,x in enumerate(temp)]\n",
    "        temp_index.sort()\n",
    "        Acc=temp[temp_index[-1][1]][0]\n",
    "        estimator=temp[temp_index[-1][1]][1]\n",
    "        Result[wid].append((Acc,deepcopy(estimator)))\n",
    "\n",
    "with open(\"Model_new_1.txt\", \"wb\") as fp:\n",
    "    pickle.dump(Result, fp)\n",
    "t2=time.time()\n",
    "print(round((t2-t1)/60) ,'min has time for calculaton Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import six\n",
    "import sys\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "from id3 import Id3Estimator, export_text\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "with open(\"Model_new_1.txt\", \"rb\") as fp:\n",
    "    Result = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9927609089081038\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "                           NAG       0.99      1.00      1.00      4896\n",
      " troca_faixa_direita_agressiva       0.96      0.59      0.73        46\n",
      "troca_faixa_esquerda_agressiva       0.78      0.68      0.72        31\n",
      "\n",
      "                      accuracy                           0.99      4973\n",
      "                     macro avg       0.91      0.75      0.82      4973\n",
      "                  weighted avg       0.99      0.99      0.99      4973\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Max_depth=7\n",
    "wid=126\n",
    "\n",
    "label=TrainData[wid].label\n",
    "data=TrainData[wid].drop('label',1)\n",
    "\n",
    "Acc=Result[wid][Max_depth-1][0]\n",
    "estimator=Result[wid][Max_depth-1][1]\n",
    "print(Acc)\n",
    "print(classification_report(label, estimator.predict(data)))\n",
    "#print(export_text(estimator.tree_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9888721804511278\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "             NAG       0.99      1.00      0.99      3234\n",
      "freada_agressiva       0.90      0.67      0.77        91\n",
      "\n",
      "        accuracy                           0.99      3325\n",
      "       macro avg       0.94      0.83      0.88      3325\n",
      "    weighted avg       0.99      0.99      0.99      3325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Max_depth=7\n",
    "wid=186\n",
    "\n",
    "label=TrainData[wid].label\n",
    "data=TrainData[wid].drop('label',1)\n",
    "\n",
    "Acc=Result[wid][Max_depth-1][0]\n",
    "estimator=Result[wid][Max_depth-1][1]\n",
    "print(Acc)\n",
    "print(classification_report(label, estimator.predict(data)))\n",
    "#print(export_text(estimator.tree_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9534528805799313\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                     NAG       0.95      1.00      0.97      2379\n",
      "    aceleracao_agressiva       0.96      0.27      0.42        81\n",
      " curva_direita_agressiva       1.00      0.52      0.68        62\n",
      "curva_esquerda_agressiva       0.99      0.69      0.81        99\n",
      "\n",
      "                accuracy                           0.95      2621\n",
      "               macro avg       0.97      0.62      0.72      2621\n",
      "            weighted avg       0.95      0.95      0.94      2621\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Max_depth=7\n",
    "wid=226\n",
    "\n",
    "label=TrainData[wid].label\n",
    "data=TrainData[wid].drop('label',1)\n",
    "\n",
    "Acc=Result[wid][Max_depth-1][0]\n",
    "estimator=Result[wid][Max_depth-1][1]\n",
    "print(Acc)\n",
    "print(classification_report(label, estimator.predict(data)))\n",
    "#print(export_text(estimator.tree_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overfit model!\n",
    "Overfit_model={\n",
    "      226:[],\n",
    "      126:[],\n",
    "      186:[],\n",
    "       }\n",
    "\n",
    "for wid in Overfit_model:\n",
    "    data=TrainData[wid].drop('label',1)\n",
    "    label=TrainData[wid].label\n",
    "    #\n",
    "    estimator = Id3Estimator(max_depth=100, min_samples_split=1, prune=True,\n",
    "                    gain_ratio=True, min_entropy_decrease=0, is_repeating=True)\n",
    "    #run it in itaraion for the best trees\n",
    "    NUMBER_OF_TEST=10\n",
    "    temp=list()\n",
    "    for NT in range(NUMBER_OF_TEST):\n",
    "        estimator.fit(data,label , check_input=True)\n",
    "        ACC=accuracy_score(y_true=label,y_pred=estimator.predict(data))\n",
    "        temp.append((ACC,deepcopy(estimator)))\n",
    "    temp_index=[(x[0],i) for i,x in enumerate(temp)]\n",
    "    temp_index.sort()\n",
    "    Acc=temp[temp_index[-1][1]][0]\n",
    "    estimator=temp[temp_index[-1][1]][1]\n",
    "    Overfit_model[wid]=(Acc,deepcopy(estimator))\n",
    "\n",
    "with open(\"TrainData_test_l_1.txt\", \"wb\") as fp:\n",
    "    pickle.dump((Overfit_model), fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wid=226\n",
    "data=TrainData[wid].drop('label',1)\n",
    "label=TrainData[wid].label\n",
    "print(classification_report(label, Overfit_model[wid][1].predict(data)))\n",
    "#its completely ovefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for in wid window:\n",
    "    estimator=Result[wid][5][1]\n",
    "    x={x for x in estimator.predict(Test_new[wid])}\n",
    "    print(f\" {wid} is : {x}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "x=data.columns[30]\n",
    "rep=re.compile(r'(\\w+)(\\d+)')\n",
    "mo=rep.findall(x)[0]\n",
    "print(mo[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot charts for find the optimom\n",
    "Y1=[x[0]for x in Result[226]]\n",
    "Y2=[x[0]for x in Result[186]]\n",
    "Y3=[x[0]for x in Result[126]]\n",
    "\n",
    "X=range(1,11)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(X,Y1, linestyle='-', linewidth=2,c='r')\n",
    "ax.plot(X,Y2, linestyle='-', linewidth=2,c='g')\n",
    "ax.plot(X,Y3, linestyle='-', linewidth=2,c='b')\n",
    "\n",
    "ax.legend();\n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

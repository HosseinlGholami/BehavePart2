{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cbook import flatten\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as matplotlib\n",
    "\n",
    "from fastdtw import fastdtw\n",
    "\n",
    "def color_map_color(value, cmap_name='coolwarm', vmin=0, vmax=10):\n",
    "    # norm = plt.Normalize(vmin, vmax)\n",
    "    norm = matplotlib.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    cmap = cm.get_cmap(cmap_name)  # PiYG\n",
    "    rgb = cmap(norm(abs(value)))[:3]  # will return rgba, we take only first 3 so we get rgb\n",
    "    color = matplotlib.colors.rgb2hex(rgb)\n",
    "    return color\n",
    "cl=['r','g','b','c','m','y','k']\n",
    "color=dict()\n",
    "for i,el in enumerate(cl):\n",
    "    color.update({i:el})\n",
    "#extract the labels for clutering precision its for after clustering\n",
    "def time_convertor(x):\n",
    "    s,h,m=0,0,0\n",
    "    s=round(x%60,2)\n",
    "    m=int(x/60)\n",
    "    h=int(m/60)\n",
    "    m=m%60\n",
    "    time=str(h)+':'+str(m)+':'+str(s)\n",
    "    return pd.to_datetime('1970-01-01 '+time)\n",
    "\n",
    "\n",
    "# Display figures inline in Jupyter notebook\n",
    "import seaborn as sns\n",
    "# Use seaborn style defaults and set the default figure size\n",
    "sns.set(rc={'figure.figsize':(15, 5)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_read\n",
    "#prepare Dataset gyroscope\n",
    "dfg=pd.read_csv('16\\giroscopio_terra.csv')\n",
    "dfg['ts']=pd.to_datetime(dfg.apply(lambda x:(round((x.uptimeNanos-dfg.uptimeNanos[0])/1000000)*1000000),axis=1))\n",
    "dm=dfg['ts']\n",
    "dfg=dfg.drop('uptimeNanos',1).drop('timestamp',1)\n",
    "dfg=dfg.set_index('ts').rename(columns={'x': 'g-x','y': 'g-y','z': 'g-z'})\n",
    "#prepare Dataset accelarator\n",
    "dfa=pd.read_csv('16\\\\acelerometro_terra.csv')\n",
    "dfa['ts']=dm\n",
    "dfa=dfa.drop('uptimeNanos',1).drop('timestamp',1)\n",
    "dfa=dfa.set_index('ts').rename(columns={'x': 'a-x','y': 'a-y','z': 'a-z'})\n",
    "dfg['g-x']=dfg.apply(lambda x:x[0]/max(dfg['g-x']),axis=1)\n",
    "dfg['g-y']=dfg.apply(lambda x:x[1]/max(dfg['g-y']),axis=1)\n",
    "dfg['g-z']=dfg.apply(lambda x:x[2]/max(dfg['g-z']),axis=1)\n",
    "dfa['a-x']=dfg.apply(lambda x:x[0]/max(dfa['a-x']),axis=1)\n",
    "dfa['a-y']=dfg.apply(lambda x:x[1]/max(dfa['a-y']),axis=1)\n",
    "dfa['a-z']=dfg.apply(lambda x:x[2]/max(dfa['a-z']),axis=1)\n",
    "df1 = pd.concat([dfa, dfg], axis=1, join='outer')\n",
    "#prepare Dataset gyroscope\n",
    "dfg=pd.read_csv('17\\giroscopio_terra.csv')\n",
    "dfg['ts']=pd.to_datetime(dfg.apply(lambda x:(round((x.uptimeNanos-dfg.uptimeNanos[0])/1000000)*1000000),axis=1))\n",
    "dm=dfg['ts']\n",
    "dfg=dfg.drop('uptimeNanos',1).drop('timestamp',1)\n",
    "dfg=dfg.set_index('ts').rename(columns={'x': 'g-x','y': 'g-y','z': 'g-z'})\n",
    "#prepare Dataset accelarator\n",
    "dfa=pd.read_csv('17\\\\acelerometro_terra.csv')\n",
    "dfa['ts']=dm\n",
    "dfa=dfa.drop('uptimeNanos',1).drop('timestamp',1)\n",
    "dfa=dfa.set_index('ts').rename(columns={'x': 'a-x','y': 'a-y','z': 'a-z'})\n",
    "dfg['g-x']=dfg.apply(lambda x:x[0]/max(dfg['g-x']),axis=1)\n",
    "dfg['g-y']=dfg.apply(lambda x:x[1]/max(dfg['g-y']),axis=1)\n",
    "dfg['g-z']=dfg.apply(lambda x:x[2]/max(dfg['g-z']),axis=1)\n",
    "dfa['a-x']=dfg.apply(lambda x:x[0]/max(dfa['a-x']),axis=1)\n",
    "dfa['a-y']=dfg.apply(lambda x:x[1]/max(dfa['a-y']),axis=1)\n",
    "dfa['a-z']=dfg.apply(lambda x:x[2]/max(dfa['a-z']),axis=1)\n",
    "df2 = pd.concat([dfa, dfg], axis=1, join='outer')\n",
    "#prepare Dataset gyroscope\n",
    "dfg=pd.read_csv('20\\\\giroscopio_terra.csv')\n",
    "dfg['ts']=pd.to_datetime(dfg.apply(lambda x:(round((x.uptimeNanos-dfg.uptimeNanos[0])/1000000)*1000000),axis=1))\n",
    "dm=dfg['ts']\n",
    "dfg=dfg.drop('uptimeNanos',1).drop('timestamp',1)\n",
    "dfg=dfg.set_index('ts').rename(columns={'x': 'g-x','y': 'g-y','z': 'g-z'})\n",
    "#prepare Dataset accelarator\n",
    "dfa=pd.read_csv('20\\\\acelerometro_terra.csv')\n",
    "dfa['ts']=dm\n",
    "dfa=dfa.drop('uptimeNanos',1).drop('timestamp',1)\n",
    "dfa=dfa.set_index('ts').rename(columns={'x': 'a-x','y': 'a-y','z': 'a-z'})\n",
    "dfg['g-x']=dfg.apply(lambda x:x[0]/max(dfg['g-x']),axis=1)\n",
    "dfg['g-y']=dfg.apply(lambda x:x[1]/max(dfg['g-y']),axis=1)\n",
    "dfg['g-z']=dfg.apply(lambda x:x[2]/max(dfg['g-z']),axis=1)\n",
    "dfa['a-x']=dfg.apply(lambda x:x[0]/max(dfa['a-x']),axis=1)\n",
    "dfa['a-y']=dfg.apply(lambda x:x[1]/max(dfa['a-y']),axis=1)\n",
    "dfa['a-z']=dfg.apply(lambda x:x[2]/max(dfa['a-z']),axis=1)\n",
    "df3 = pd.concat([dfa, dfg], axis=1, join='outer')\n",
    "#prepare Dataset gyroscope\n",
    "dfg=pd.read_csv('21\\giroscopio_terra.csv')\n",
    "dfg['ts']=pd.to_datetime(dfg.apply(lambda x:(round((x.uptimeNanos-dfg.uptimeNanos[0])/1000000)*1000000),axis=1))\n",
    "dm=dfg['ts']\n",
    "dfg=dfg.drop('uptimeNanos',1).drop('timestamp',1)\n",
    "dfg=dfg.set_index('ts').rename(columns={'x': 'g-x','y': 'g-y','z': 'g-z'})\n",
    "#prepare Dataset accelarator\n",
    "dfa=pd.read_csv('21\\\\acelerometro_terra.csv')\n",
    "dfa['ts']=dm\n",
    "dfa=dfa.drop('uptimeNanos',1).drop('timestamp',1)\n",
    "dfa=dfa.set_index('ts').rename(columns={'x': 'a-x','y': 'a-y','z': 'a-z'})\n",
    "dfg['g-x']=dfg.apply(lambda x:x[0]/max(dfg['g-x']),axis=1)\n",
    "dfg['g-y']=dfg.apply(lambda x:x[1]/max(dfg['g-y']),axis=1)\n",
    "dfg['g-z']=dfg.apply(lambda x:x[2]/max(dfg['g-z']),axis=1)\n",
    "dfa['a-x']=dfg.apply(lambda x:x[0]/max(dfa['a-x']),axis=1)\n",
    "dfa['a-y']=dfg.apply(lambda x:x[1]/max(dfa['a-y']),axis=1)\n",
    "dfa['a-z']=dfg.apply(lambda x:x[2]/max(dfa['a-z']),axis=1)\n",
    "df4 = pd.concat([dfa, dfg], axis=1, join='outer')\n",
    "label_event_lenght=dict()\n",
    "#prepare Labeled dataset on 16\n",
    "dfl=pd.read_csv('16\\\\groundTruth.csv')\n",
    "dfl['length']=dfl.en-dfl.st\n",
    "for event , dft in dfl.groupby('evento'):\n",
    "    if (event in label_event_lenght):\n",
    "        label_event_lenght.update({event : label_event_lenght[event]+list(dft.length) })\n",
    "    else:\n",
    "        label_event_lenght.update({event : list(dft.length)})\n",
    "        \n",
    "dfl['st_time']=dfl.apply(lambda x:time_convertor(x.st) ,axis=1 )\n",
    "dfl['en_time']=dfl.apply(lambda x:time_convertor(x.en) ,axis=1 )\n",
    "dfl=dfl.drop('st',1).drop('en',1).drop('length',1)\n",
    "evented_label_1=list()\n",
    "for i in range(len(dfl)):\n",
    "    evented_label_1.append((dfl.iloc[i][0],dfl.iloc[i][1],dfl.iloc[i][2]))\n",
    "#prepare Labeled dataset on 17\n",
    "dfl=pd.read_csv('17\\\\groundTruth.csv')\n",
    "dfl['length']=dfl.en-dfl.st\n",
    "for event , dft in dfl.groupby('evento'):\n",
    "    if (event in label_event_lenght):\n",
    "        label_event_lenght.update({event : label_event_lenght[event]+list(dft.length) })\n",
    "    else:\n",
    "        label_event_lenght.update({event : list(dft.length)})\n",
    "\n",
    "dfl['st_time']=dfl.apply(lambda x:time_convertor(x.st) ,axis=1 )\n",
    "dfl['en_time']=dfl.apply(lambda x:time_convertor(x.en) ,axis=1 )\n",
    "dfl=dfl.drop('st',1).drop('en',1).drop('length',1)\n",
    "evented_label_2=list()\n",
    "for i in range(len(dfl)):\n",
    "    evented_label_2.append((dfl.iloc[i][0],dfl.iloc[i][1],dfl.iloc[i][2]))\n",
    "#prepare Labeled dataset on 20\n",
    "dfl=pd.read_csv('20\\\\groundTruth.csv')\n",
    "dfl['length']=dfl.en-dfl.st\n",
    "for event , dft in dfl.groupby('evento'):\n",
    "    if (event in label_event_lenght):\n",
    "        label_event_lenght.update({event : label_event_lenght[event]+list(dft.length) })\n",
    "    else:\n",
    "        label_event_lenght.update({event : list(dft.length)})\n",
    "\n",
    "dfl['st_time']=dfl.apply(lambda x:time_convertor(x.st) ,axis=1 )\n",
    "dfl['en_time']=dfl.apply(lambda x:time_convertor(x.en) ,axis=1 )\n",
    "dfl=dfl.drop('st',1).drop('en',1).drop('length',1)\n",
    "evented_label_3=list()\n",
    "for i in range(len(dfl)):\n",
    "    evented_label_3.append((dfl.iloc[i][0],dfl.iloc[i][1],dfl.iloc[i][2]))\n",
    "#prepare Labeled dataset on 21\n",
    "dfl=pd.read_csv('21\\\\groundTruth.csv')\n",
    "dfl['length']=dfl.en-dfl.st\n",
    "for event , dft in dfl.groupby('evento'):\n",
    "    if (event in label_event_lenght):\n",
    "        label_event_lenght.update({event : label_event_lenght[event]+list(dft.length) })\n",
    "    else:\n",
    "        label_event_lenght.update({event : list(dft.length)})\n",
    "dfl['st_time']=dfl.apply(lambda x:time_convertor(x.st) ,axis=1 )\n",
    "dfl['en_time']=dfl.apply(lambda x:time_convertor(x.en) ,axis=1 )\n",
    "dfl=dfl.drop('st',1).drop('en',1).drop('length',1)\n",
    "evented_label_4=list()\n",
    "for i in range(len(dfl)):\n",
    "    evented_label_4.append((dfl.iloc[i][0],dfl.iloc[i][1],dfl.iloc[i][2]))\n",
    "eve1=pd.DataFrame(evented_label_1)\n",
    "eve1[3]=['evented_label_1' for x in evented_label_1]\n",
    "\n",
    "eve2=pd.DataFrame(evented_label_2)\n",
    "eve2[3]=['evented_label_2' for x in evented_label_2]\n",
    "\n",
    "eve3=pd.DataFrame(evented_label_3)\n",
    "eve3[3]=['evented_label_3' for x in evented_label_3]\n",
    "\n",
    "eve4=pd.DataFrame(evented_label_4)\n",
    "eve4[3]=['evented_label_4' for x in evented_label_4]\n",
    "\n",
    "evented_label_All=pd.concat([eve1, eve2,eve3,eve4])\n",
    "#supervised\n",
    "window={226:['curva_direita_agressiva','curva_esquerda_agressiva','aceleracao_agressiva'],\n",
    "        126:['troca_faixa_direita_agressiva','troca_faixa_esquerda_agressiva'],\n",
    "        186:['freada_agressiva']\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extraction  1.0  % of  3  is complete\n"
     ]
    }
   ],
   "source": [
    "#swich to thease data set for finding labels\n",
    "with open(\"Data/data_list_per_driver\", \"rb\") as fp:\n",
    "        Sensory_data = pickle.load(fp)\n",
    "\n",
    "Lambda=1\n",
    "sample_rate=1#0.04/0.02\n",
    "t1=time.time()\n",
    "def TempEvent_genarator_new(Lambda,df,L,i):\n",
    "    M=int((L)*Lambda)\n",
    "    temp_event=list()\n",
    "    pointer=0\n",
    "    stop=False\n",
    "    #go on timeserise for event extraction\n",
    "    while stop!=True:\n",
    "        clear_output(wait=True)\n",
    "        pointer=pointer + M\n",
    "        l_min=pointer-int(L/2)\n",
    "        l_max=pointer+int(L/2)\n",
    "        if l_max<0:\n",
    "            l_max=L/2\n",
    "        if (l_min<0):\n",
    "            l_min=0\n",
    "        if (l_max>len(df)):\n",
    "            l_max=len(df)\n",
    "            stop=True\n",
    "        print('extraction ',l_max/len(df),' % of ',i,' is complete')\n",
    "        temp_event.append(df[l_min:l_max])\n",
    "    return temp_event\n",
    "\n",
    "temp_event_test_new=list()\n",
    "for l,DF in enumerate(Sensory_data):\n",
    "    te=list()\n",
    "    DF['g-x']=DF.apply(lambda x:x[0]/max(DF['g-x']),axis=1)\n",
    "    DF['g-y']=DF.apply(lambda x:x[2]/max(DF['g-y']),axis=1)\n",
    "    DF['g-z']=DF.apply(lambda x:x[1]/max(DF['g-z']),axis=1)\n",
    "    DF['a-x']=DF.apply(lambda x:x[3]/max(DF['a-x']),axis=1)\n",
    "    DF['a-y']=DF.apply(lambda x:x[5]/max(DF['a-y']),axis=1)\n",
    "    DF['a-z']=DF.apply(lambda x:x[4]/max(DF['a-z']),axis=1)\n",
    "    for i,lw in enumerate(window):\n",
    "        te.append(TempEvent_genarator_new(Lambda,DF,lw/sample_rate,i+1))\n",
    "    temp_event_test_new.append(te)\n",
    "\n",
    "#make it dataframe\n",
    "Data_test_new={\n",
    "      226:[],\n",
    "      126:[],\n",
    "      186:[],\n",
    "       }\n",
    "for driver in range(len(temp_event_test_new)):\n",
    "    Data_test_new[226]=Data_test_new[226]+[ x for x in temp_event_test_new[driver][0] ]\n",
    "    Data_test_new[186]=Data_test_new[186]+[ x for x in temp_event_test_new[driver][1] ]\n",
    "    Data_test_new[126]=Data_test_new[126]+[ x for x in temp_event_test_new[driver][2] ]\n",
    "    \n",
    "    \n",
    "#calculate distance\n",
    "k,n=0,0\n",
    "num_event=dict(evented_label_All.groupby(0).size())\n",
    "for lenght in window:\n",
    "    for name in window[lenght]:\n",
    "        k=k+len(Data_test_new[lenght])*num_event[name]\n",
    "\n",
    "Data_DS_test_new={\n",
    "      226:[],\n",
    "      126:[],\n",
    "      186:[],\n",
    "       }\n",
    "\n",
    "for wind in Data_test_new:\n",
    "    for event in Data_test_new[wind]:\n",
    "         #---------for each Data-distance from events----------\n",
    "        Y=list()\n",
    "        for from_event,dfe in evented_label_All.groupby(0):\n",
    "            if from_event in window[wind]:\n",
    "                X=list()\n",
    "                #____for each event_distance from each label_____\n",
    "                for el in dfe.iloc:\n",
    "                    clear_output(wait=True)\n",
    "                    n=n+1\n",
    "                    print('disstance calculatoin ',round((n/k)*100,4),' %  is complete')\n",
    "                    if   el[3]=='evented_label_1':\n",
    "                        X.append(fastdtw(event,df1[el[1]:el[2]])[0])\n",
    "                    elif el[3]=='evented_label_2':\n",
    "                        X.append(fastdtw(event,df2[el[1]:el[2]])[0])\n",
    "                    elif el[3]=='evented_label_3':\n",
    "                        X.append(fastdtw(event,df3[el[1]:el[2]])[0])\n",
    "                    elif el[3]=='evented_label_4':\n",
    "                        X.append(fastdtw(event,df4[el[1]:el[2]])[0])\n",
    "                #_________________________________________________\n",
    "                Y.append((from_event,X))\n",
    "        Data_DS_test_new[wind].append(Y)\n",
    "        #------------------------------------------------------\n",
    "#prepare dataset for labeling\n",
    "Test_new=dict()\n",
    "for win in Data_DS_test_new:\n",
    "    dumy_list=list()\n",
    "    for events in Data_DS_test_new[win]:\n",
    "        dumy_dict=dict()\n",
    "        for name , event in events:\n",
    "            for tag,number in enumerate(event):\n",
    "                dumy_dict.update({name+str(tag):number})\n",
    "        dumy_list.append(dumy_dict)\n",
    "    Test_new.update({win:pd.DataFrame(dumy_list)})\n",
    "t2=time.time()\n",
    "print(round((t2-t1)/60) ,'min has time left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import six\n",
    "import sys\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "from id3 import Id3Estimator, export_text\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "with open(\"Model_new_1.txt\", \"rb\") as fp:\n",
    "    Result = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_depth=8\n",
    "wid=226\n",
    "6\n",
    "\n",
    "estimator=Result[wid][Max_depth-1][1]\n",
    "\n",
    "{x for x in estimator.predict(Test_new[wid])}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
